# -*- coding: utf-8 -*-
"""topic model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_WregeLhQvV84eJQk7igWl2iEH_ZzqR
"""

pip install gensim

import pandas as pd
import gensim
from gensim import corpora
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

nltk.download('stopwords')
nltk.download('wordnet')

pip install kagglehub[pandas-datasets]

import kagglehub
from kagglehub import KaggleDatasetAdapter

file_path = "bbc_data.csv"

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "alfathterry/bbc-full-text-document-classification",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

print("First 5 records:", df.head())

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower().translate(str.maketrans('', '', string.punctuation))
    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words and len(w) > 3]
    return tokens

processed_docs = df['data'].apply(clean_text)

dictionary = corpora.Dictionary(processed_docs)

corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from gensim.models import LdaModel, Nmf
from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora import Dictionary

dictionary = Dictionary(processed_docs)
corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
num_topics = 6

lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=20)
coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')
lda_score = coherence_model_lda.get_coherence()

nmf_model = Nmf(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=15)
coherence_model_nmf = CoherenceModel(model=nmf_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')
nmf_score = coherence_model_nmf.get_coherence()

print(f"LDA Coherence Score: {lda_score}")
print(f"NMF Coherence Score: {nmf_score}")

pip install pyLDAvis

warnings.filterwarnings("ignore", category=DeprecationWarning)
import pyLDAvis.gensim_models
import pyLDAvis

vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)
pyLDAvis.save_html(vis, 'lda_visualization.html')
print("da_visualization.html")

"""Technical Note: Resolving `AttributeError` for NMF Models in pyLDAvis
The Problem:
I encountered an issue when using `pyLDAvis.gensim_models.prepare` with my **NMF** model, resulting in the error: `AttributeError: 'Nmf' object has no attribute 'inference'`. This occurs because the library's built-in utility is optimized for LDA models and expects an `.inference()` method which is not present in NMF's matrix factorization logic.

The Solution (Manual Preparation):
To resolve this, I bypassed the automated utility and prepared the data manually through the following steps:

1. Normalization:I converted the NMF raw weights into probabilities (ensuring each row sums to 1.0) so the visualizer can correctly interpret the topic-term and document-topic distributions.
2. Dense Matrix Conversion: I transformed the sparse output from the Gensim NMF model into dense NumPy arrays to ensure compatibility with the visualization engine.
3. Manual Feeding:I provided the processed matrices directly to `pyLDAvis.prepare`, successfully skipping the requirement for the missing `.inference` attribute.

The Result:
By implementing this manual workflow, I achieved a stable and highly interpretable visualization of the NMF topics, overcoming the libraryâ€™s default compatibility constraints.
"""

import numpy as np
import pyLDAvis


topic_term_dists = nmf_model.get_topics()

topic_term_dists /= topic_term_dists.sum(axis=1)[:, None]


doc_topic_dists = []
for bow in corpus:
    topic_dist = dict(nmf_model[bow])
    full_dist = [topic_dist.get(i, 0.0) for i in range(nmf_model.num_topics)]
    doc_topic_dists.append(full_dist)

doc_topic_dists = np.array(doc_topic_dists)
doc_topic_dists /= doc_topic_dists.sum(axis=1)[:, None]

doc_lengths = [sum(cnt for _, cnt in bow) for bow in corpus]
vocab = [dictionary[i] for i in range(len(dictionary))]
term_freqs = [dictionary.cfs[i] for i in range(len(dictionary))]

vis_data = pyLDAvis.prepare(
    topic_term_dists=topic_term_dists,
    doc_topic_dists=doc_topic_dists,
    doc_lengths=doc_lengths,
    vocab=vocab,
    term_frequency=term_freqs,
    sort_topics=False
)
pyLDAvis.save_html(vis_data, 'nmf_visualization_fixed.html')
print("nmf_visualization_fixed.html")